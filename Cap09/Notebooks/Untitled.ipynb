{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function polyfit in module numpy:\n",
      "\n",
      "polyfit(x, y, deg, rcond=None, full=False, w=None, cov=False)\n",
      "    Least squares polynomial fit.\n",
      "    \n",
      "    Fit a polynomial ``p(x) = p[0] * x**deg + ... + p[deg]`` of degree `deg`\n",
      "    to points `(x, y)`. Returns a vector of coefficients `p` that minimises\n",
      "    the squared error in the order `deg`, `deg-1`, ... `0`.\n",
      "    \n",
      "    The `Polynomial.fit <numpy.polynomial.polynomial.Polynomial.fit>` class\n",
      "    method is recommended for new code as it is more stable numerically. See\n",
      "    the documentation of the method for more information.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    x : array_like, shape (M,)\n",
      "        x-coordinates of the M sample points ``(x[i], y[i])``.\n",
      "    y : array_like, shape (M,) or (M, K)\n",
      "        y-coordinates of the sample points. Several data sets of sample\n",
      "        points sharing the same x-coordinates can be fitted at once by\n",
      "        passing in a 2D-array that contains one dataset per column.\n",
      "    deg : int\n",
      "        Degree of the fitting polynomial\n",
      "    rcond : float, optional\n",
      "        Relative condition number of the fit. Singular values smaller than\n",
      "        this relative to the largest singular value will be ignored. The\n",
      "        default value is len(x)*eps, where eps is the relative precision of\n",
      "        the float type, about 2e-16 in most cases.\n",
      "    full : bool, optional\n",
      "        Switch determining nature of return value. When it is False (the\n",
      "        default) just the coefficients are returned, when True diagnostic\n",
      "        information from the singular value decomposition is also returned.\n",
      "    w : array_like, shape (M,), optional\n",
      "        Weights to apply to the y-coordinates of the sample points. For\n",
      "        gaussian uncertainties, use 1/sigma (not 1/sigma**2).\n",
      "    cov : bool or str, optional\n",
      "        If given and not `False`, return not just the estimate but also its\n",
      "        covariance matrix. By default, the covariance are scaled by\n",
      "        chi2/sqrt(N-dof), i.e., the weights are presumed to be unreliable\n",
      "        except in a relative sense and everything is scaled such that the\n",
      "        reduced chi2 is unity. This scaling is omitted if ``cov='unscaled'``,\n",
      "        as is relevant for the case that the weights are 1/sigma**2, with\n",
      "        sigma known to be a reliable estimate of the uncertainty.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    p : ndarray, shape (deg + 1,) or (deg + 1, K)\n",
      "        Polynomial coefficients, highest power first.  If `y` was 2-D, the\n",
      "        coefficients for `k`-th data set are in ``p[:,k]``.\n",
      "    \n",
      "    residuals, rank, singular_values, rcond\n",
      "        Present only if `full` = True.  Residuals is sum of squared residuals\n",
      "        of the least-squares fit, the effective rank of the scaled Vandermonde\n",
      "        coefficient matrix, its singular values, and the specified value of\n",
      "        `rcond`. For more details, see `linalg.lstsq`.\n",
      "    \n",
      "    V : ndarray, shape (M,M) or (M,M,K)\n",
      "        Present only if `full` = False and `cov`=True.  The covariance\n",
      "        matrix of the polynomial coefficient estimates.  The diagonal of\n",
      "        this matrix are the variance estimates for each coefficient.  If y\n",
      "        is a 2-D array, then the covariance matrix for the `k`-th data set\n",
      "        are in ``V[:,:,k]``\n",
      "    \n",
      "    \n",
      "    Warns\n",
      "    -----\n",
      "    RankWarning\n",
      "        The rank of the coefficient matrix in the least-squares fit is\n",
      "        deficient. The warning is only raised if `full` = False.\n",
      "    \n",
      "        The warnings can be turned off by\n",
      "    \n",
      "        >>> import warnings\n",
      "        >>> warnings.simplefilter('ignore', np.RankWarning)\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    polyval : Compute polynomial values.\n",
      "    linalg.lstsq : Computes a least-squares fit.\n",
      "    scipy.interpolate.UnivariateSpline : Computes spline fits.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The solution minimizes the squared error\n",
      "    \n",
      "    .. math ::\n",
      "        E = \\sum_{j=0}^k |p(x_j) - y_j|^2\n",
      "    \n",
      "    in the equations::\n",
      "    \n",
      "        x[0]**n * p[0] + ... + x[0] * p[n-1] + p[n] = y[0]\n",
      "        x[1]**n * p[0] + ... + x[1] * p[n-1] + p[n] = y[1]\n",
      "        ...\n",
      "        x[k]**n * p[0] + ... + x[k] * p[n-1] + p[n] = y[k]\n",
      "    \n",
      "    The coefficient matrix of the coefficients `p` is a Vandermonde matrix.\n",
      "    \n",
      "    `polyfit` issues a `RankWarning` when the least-squares fit is badly\n",
      "    conditioned. This implies that the best fit is not well-defined due\n",
      "    to numerical error. The results may be improved by lowering the polynomial\n",
      "    degree or by replacing `x` by `x` - `x`.mean(). The `rcond` parameter\n",
      "    can also be set to a value smaller than its default, but the resulting\n",
      "    fit may be spurious: including contributions from the small singular\n",
      "    values can add numerical noise to the result.\n",
      "    \n",
      "    Note that fitting polynomial coefficients is inherently badly conditioned\n",
      "    when the degree of the polynomial is large or the interval of sample points\n",
      "    is badly centered. The quality of the fit should always be checked in these\n",
      "    cases. When polynomial fits are not satisfactory, splines may be a good\n",
      "    alternative.\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Wikipedia, \"Curve fitting\",\n",
      "           https://en.wikipedia.org/wiki/Curve_fitting\n",
      "    .. [2] Wikipedia, \"Polynomial interpolation\",\n",
      "           https://en.wikipedia.org/wiki/Polynomial_interpolation\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> import warnings\n",
      "    >>> x = np.array([0.0, 1.0, 2.0, 3.0,  4.0,  5.0])\n",
      "    >>> y = np.array([0.0, 0.8, 0.9, 0.1, -0.8, -1.0])\n",
      "    >>> z = np.polyfit(x, y, 3)\n",
      "    >>> z\n",
      "    array([ 0.08703704, -0.81349206,  1.69312169, -0.03968254]) # may vary\n",
      "    \n",
      "    It is convenient to use `poly1d` objects for dealing with polynomials:\n",
      "    \n",
      "    >>> p = np.poly1d(z)\n",
      "    >>> p(0.5)\n",
      "    0.6143849206349179 # may vary\n",
      "    >>> p(3.5)\n",
      "    -0.34732142857143039 # may vary\n",
      "    >>> p(10)\n",
      "    22.579365079365115 # may vary\n",
      "    \n",
      "    High-order polynomials may oscillate wildly:\n",
      "    \n",
      "    >>> with warnings.catch_warnings():\n",
      "    ...     warnings.simplefilter('ignore', np.RankWarning)\n",
      "    ...     p30 = np.poly1d(np.polyfit(x, y, 30))\n",
      "    ...\n",
      "    >>> p30(4)\n",
      "    -0.80000000000000204 # may vary\n",
      "    >>> p30(5)\n",
      "    -0.99999999999999445 # may vary\n",
      "    >>> p30(4.5)\n",
      "    -0.10547061179440398 # may vary\n",
      "    \n",
      "    Illustration:\n",
      "    \n",
      "    >>> import matplotlib.pyplot as plt\n",
      "    >>> xp = np.linspace(-2, 6, 100)\n",
      "    >>> _ = plt.plot(x, y, '.', xp, p(xp), '-', xp, p30(xp), '--')\n",
      "    >>> plt.ylim(-2,2)\n",
      "    (-2, 2)\n",
      "    >>> plt.show()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.polyfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
